"""
Postgres-backed YStore for pycrdt (per-room).
Matches the y_updates / y_snapshots schema we created.
"""

from typing import AsyncIterator, Optional, Tuple

import asyncpg
from django.conf import settings
from django.utils import timezone
from pycrdt.store import BaseYStore

from backend.utils import log_debug, log_error, log_info

from .tasks import sync_snapshot_with_page


class PostgresYStore(BaseYStore):
    """
    A YStore bound to a single room (doc). Create one instance per room_id.

    Tables:

      y_updates (
        id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        room_id VARCHAR(255) NOT NULL,
        yupdate BYTEA NOT NULL,
        timestamp TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
      )
      -- indexes: (room_id, id) and optional BRIN(timestamp)

      y_snapshots (
        room_id VARCHAR(255) PRIMARY KEY,
        snapshot BYTEA NOT NULL,
        last_update_id BIGINT NOT NULL,
        timestamp TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
      )
    """

    def __init__(self, room_id: str, db_config: dict):
        # BaseYStore.__init__ expects (path, metadata_callback, log)
        super().__init__(path=room_id, metadata_callback=None, log=None)
        self.room_id = room_id
        self.db_config = db_config
        self.pool: Optional[asyncpg.Pool] = None

    # ---- pool lifecycle ----------------------------------------------------

    async def initialize_pool(self) -> None:
        """Initialize asyncpg pool. Call once before first use."""
        if self.pool is None:
            self.pool = await asyncpg.create_pool(
                host=self.db_config["host"],
                port=int(self.db_config["port"]),
                user=self.db_config["user"],
                password=self.db_config["password"],
                database=self.db_config["name"],
                min_size=1,
                max_size=10,
            )

    async def close_pool(self) -> None:
        """Close pool. Call on room close / app shutdown."""
        if self.pool:
            await self.pool.close()
            self.pool = None

    # ---- required BaseYStore API (per-room) --------------------------------

    async def write(self, data: bytes) -> None:
        """Append a CRDT update for this room."""
        assert self.pool is not None, "PostgresYStore.initialize_pool() not called"

        try:
            log_debug(f"Writing {len(data)} bytes to y_updates for room={self.room_id}")
            async with self.pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO y_updates (room_id, yupdate)
                    VALUES ($1, $2)
                    """,
                    self.room_id,
                    data,
                )
                log_debug(f"Successfully persisted {len(data)} bytes for room={self.room_id}")
        except Exception as e:
            log_error(f"Error writing to y_updates: {e}", exc_info=True)
            raise

    async def read(self) -> AsyncIterator[Tuple[bytes, bytes, float]]:
        """
        Stream all updates for this room in strict order.
        Yields (update_bytes, metadata_bytes, unix_ts_float).
        """
        assert self.pool is not None, "PostgresYStore.initialize_pool() not called"
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT yupdate, EXTRACT(EPOCH FROM timestamp) AS ts
                FROM y_updates
                WHERE room_id = $1
                ORDER BY id
                """,
                self.room_id,
            )
            for row in rows:
                yield (bytes(row["yupdate"]), b"", float(row["ts"]))

    # ---- optional helpers for snapshots / compaction -----------------------

    async def read_since(self, last_inclusive_id: int) -> AsyncIterator[Tuple[bytes, bytes, float, int]]:
        """
        Stream updates with id > last_inclusive_id.
        Yields (update_bytes, metadata_bytes, unix_ts_float, id).
        """
        assert self.pool is not None, "PostgresYStore.initialize_pool() not called"
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT id, yupdate, EXTRACT(EPOCH FROM timestamp) AS ts
                FROM y_updates
                WHERE room_id = $1 AND id > $2
                ORDER BY id
                """,
                self.room_id,
                int(last_inclusive_id),
            )
            for row in rows:
                yield (bytes(row["yupdate"]), b"", float(row["ts"]), int(row["id"]))

    async def get_max_update_id(self) -> Optional[int]:
        assert self.pool is not None, "PostgresYStore.initialize_pool() not called"
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT MAX(id) AS max_id FROM y_updates WHERE room_id = $1",
                self.room_id,
            )
            return None if not row or row["max_id"] is None else int(row["max_id"])

    async def upsert_snapshot(self, snapshot: bytes, last_update_id: int) -> None:
        """Write or update the snapshot row for this room."""
        assert self.pool is not None, "PostgresYStore.initialize_pool() not called"
        async with self.pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO y_snapshots (room_id, snapshot, last_update_id)
                VALUES ($1, $2, $3)
                ON CONFLICT (room_id)
                DO UPDATE SET
                    snapshot = EXCLUDED.snapshot,
                    last_update_id = EXCLUDED.last_update_id,
                    timestamp = CURRENT_TIMESTAMP
                """,
                self.room_id,
                snapshot,
                int(last_update_id),
            )

        sync_snapshot_with_page.enqueue(self.room_id, schedule=timezone.timedelta(seconds=0.5))

    async def get_snapshot(self) -> Optional[Tuple[bytes, int]]:
        """Return (snapshot_bytes, last_update_id) or None."""
        assert self.pool is not None, "PostgresYStore.initialize_pool() not called"
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT snapshot, last_update_id FROM y_snapshots WHERE room_id = $1",
                self.room_id,
            )
            if not row:
                return None
            return (bytes(row["snapshot"]), int(row["last_update_id"]))

    async def delete_updates_before_snapshot(self) -> int:
        """
        Delete all y_updates rows with id <= last_update_id in snapshot.
        Returns count of deleted rows.

        Safety: Only deletes if snapshot exists for this room.
        This is safe because the snapshot contains all updates up to last_update_id,
        so those updates are redundant.
        """
        assert self.pool is not None, "PostgresYStore.initialize_pool() not called"
        async with self.pool.acquire() as conn:
            # Get snapshot's watermark
            row = await conn.fetchrow(
                "SELECT last_update_id FROM y_snapshots WHERE room_id = $1",
                self.room_id,
            )
            if not row:
                log_debug("No snapshot exists for %s, skipping cleanup", self.room_id)
                return 0

            last_update_id = row["last_update_id"]

            # Delete old updates (they're already in the snapshot)
            result = await conn.execute(
                """
                DELETE FROM y_updates
                WHERE room_id = $1 AND id <= $2
                """,
                self.room_id,
                last_update_id,
            )

            # Parse result like "DELETE 1234"
            deleted_count = int(result.split()[-1]) if result else 0
            log_info(
                "Cleanup for %s: deleted %s updates up to id %s",
                self.room_id,
                deleted_count,
                last_update_id,
            )
            return deleted_count

    # ---- metadata (unused) --------------------------------------------------

    async def get_metadata(self) -> bytes:
        return b""

    async def write_metadata(self, metadata: bytes) -> None:
        return None


def get_db_config_from_django() -> dict:
    """Read Postgres settings from Django settings.DATABASES['default']."""
    db = settings.DATABASES["default"]
    return {
        "host": db.get("HOST", "localhost"),
        "port": int(db.get("PORT") or 5432),
        "user": db.get("USER", "postgres"),
        "password": db.get("PASSWORD", ""),
        "name": db.get("NAME", "postgres"),
    }
